I miss like how my family used to gather like at the end of the day how we used to talk my home was like a normal H the simple daily details that everyone has heo lived here in Northern Gaza her family evacuated on October 11th 2023 by February she learned her home was no longer there she talked to me from a friend's home in Rafa in southern Gaza we received a picture of our house and we weren shooked we had down there like a place where we have a trees we have flowers planted Hiba didn't know exactly why her home had been destroyed but over the past few months Israeli journalists have found that much of the destruction in Gaza since the attacks of October 7th has been enabled and often directed by an artificial intelligence system the promise of AI generally is a promise of in two respects one is swiftness and the second is accuracy the whole dream of AI is that it would offer these Precision strikes but after over 34,000 Palestinians killed compared to just over 1,400 in Israel's 2014 war in Gaza it's clear something different is happening so what does AI have to do with it to get some answers we called a couple of AI experts reporters and investigative journalists [Music] the Israeli Defense Forces use of AI is not new I think that the most famous use of AI by the IDF is of course the iron doome which is a defensive system that aims to disrupt H the threat of missile attacks this system is partly what defended Israel against Iran's drone and missile attacks in April 2024 the other one is another homegrown weapon that they have called the smash from Smart shooter which is an AI Precision assault rifle site that you add on to handheld weapons and what it does is it uses Advanced image processing algorithms to hone in on a Target sort of like a uh an auto aim in Call of Duty another way Israel uses AI is through surveillance of Palestinians in the occupied territories every time they pass through one of the the hundreds of checkpoints their movements are being registered their facial images and other Biometrics are being matched against a database but we're now learning more about the AI systems that choose bombing Targets in Gaza from two reports in the Israeli Publications 972 and local call gospel is a system that produces bombing targets for specific buildings and structures in Gaza it does this by working in conjunction with other AI tools and like any AI system the first step is the large-scale collection of data in this case surveillance and historical data on Palestinians and militant locations in Gaza the most famous application will be The Alchemist which is a platform that collects data and allows the transfer of data between different departments later being transferred to another platform which is called the fire Factory the fire Factory observes the data and categorizes it the generated targets are generally put into one of four categories first tactical targets which usually include armed militant cells weapons warehouses launchers and militant headquarters then there are underground targets primarily tunnels under civilian homes the third category includes the family homes of Hamas or Islamic Jihad operatives and the last category includes targets that are not obviously military nature particularly residential and high-rise buildings with dozens of civilians the IDF calls these power targets once the data is organized it goes through a third layer called The Gospel the gospel creates an output which suggests specific possible targets possible Munitions warnings of possible collateral damage and Etc this system produces Targets in Gaza faster than a human can and within the first 5 days of the war half of all the targets identified were from the power targets category multiple sources who spoke to 972 reported that the idea behind power targets is to exert civil pressure on Hamas hea's home was most likely one of the power targets picked up by the gospel system months after the gospel investigation 972 also surfaced a more opaque and secretive AI system built for targeting specific people known as lavender as the Israel Hamas War Began lavender used historic data and surveillance to generate as many as 37,000 Hamas and Islamic Jihad targets sources told 972 that about 10% of those targets are often wrong but even when determining the 90% of supposedly correct targets Israel also expanded the definition of a Hamas operative for the first time the thing is Hamas ultimately runs the Gaza Strip so you have a lot of civil society that interacts with Hamas police force doctor Civil Society in in general and so these are the targets that we know that they're looking at after lavender used its data to generate these targets AI would then link the target to a specific family home and then recommend a weapon for the IDF to use on the target mostly depending on the ranking of the operative what we were told is that for low ranking kabas militants the Army preferred to use dumb bombs meaning bombs that are not guid it because they are cheaper so in a in a strange way the less of the danger you pose then they used a less sophisticated uh bombs and therefore maybe creating more collateral damage sources told reporters that for every Junior Hamas operative that lavender marked it was permissible to kill up to 15 or 20 civilians but also that for some targets the number of permissible civilian casualties was as high as 300 AI systems do not produce facts they only produce prediction just like a weather forecast or the stock market the intelligence that's there is completely dependent on the quality the validity the the understanding of the humans who created the system in a statement to the guardian the IDF outright rejected that they had any policy to kill tens of thousands of people in their homes and stressed that human analysts must conduct independent examinations before a Target is selected which brings us to the last step of both of these processes human approval sources told 972 that the only human supervision protocol in place before bombing the houses of suspected Junior militants marked by lavender was to conduct a single check insurance ing that the AI selected Target is male rather than female experts have been telling us that essentially what's happening in Gaza is an unwilling test site for future AI Technologies in November 2023 the US released an international framework for their responsible use of AI in war more than 50 signatures from 50 different countries Tre Israel has not signed on to this treaty so we're in sort of this space where we lack sufficient oversight and accountability for drone Warfare let alone new systems being introduced like gospel and lavender and we're looking at a future really where there is going to be more imprecise and biased automation of targets that make these civilian casualties much worse the fallacy of you know the premise that faster War fighting is somehow going to lead to Global Security and peace I mean this is just not the path um that's going to get us there and on the contrary I think a lot of these uh the momentum of these technological initiatives needs to be interrupted in in whatever ways we can it it really aches my heart that these woms are never going to be back it's not like I left home and like for example I traveled and like I know it's there no it's not it's not there anymore